# --- 1. Imports ---
import streamlit as st
import openai
import json
from typing import Dict, List
from dotenv import load_dotenv
import os

# Import the semantic search function from our utility module.
from contracts_utils import search_contract

# --- 2. Configuration and Setup ---
load_dotenv()
openai_api_key = os.environ.get('OPENAI_API_KEY')  # Use .get() for a friendlier error message

# --- Critical: Check for API Key ---
if not openai_api_key:
    st.error("Your OPENAI_API_KEY is not set. Please create a .env file with your key.")
    st.stop()  # Stop the app if the key is missing

# Initialize the OpenAI client
client = openai.Client(api_key=openai_api_key)

# Configure the Streamlit page
st.set_page_config(page_title="Chat Completions Chatbot", page_icon="üìù")

# --- 3. Streamlit Session State Initialization ---
# The message history is now the ONLY source of truth for the conversation's context.
if 'messages' not in st.session_state:
    st.session_state.messages = [
        # We can optionally add a system message to set the AI's persona and instructions.
        {"role": "system",
         "content": "You are a helpful assistant who can answer questions about insurance contracts. When you need to find a contract, use the available tools."}
    ]


# --- 4. Tool Definition ---
# This is the local Python function that the AI model can choose to call.
def semantic_contract_search(query: str) -> List[Dict]:
    """
    Finds relevant contracts based on a semantic query. This function is made
    available to the OpenAI model as a tool.

    Args:
        query (str): The search query generated by the AI model.

    Returns:
        A list of JSON strings, where each string is a contract document.
    """
    print(f"Executing semantic search with query: '{query}'")
    return search_contract(query)


# This dictionary maps the tool name (as known by the AI) to our actual Python function.
available_tools = {
    "semantic_contract_search": semantic_contract_search,
}

# This is the JSON schema that describes our tool to the OpenAI API.
# The model uses this schema to understand what the tool does and what arguments it needs.
tools = [
    {
        "type": "function",
        "function": {
            "name": "semantic_contract_search",
            "description": "Searches for and retrieves insurance contracts based on a user's query about its content.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The specific query to search for in the contract database. For example, 'water damage claims'.",
                    },
                },
                "required": ["query"],
            },
        },
    }
]


# --- 5. Chat Completions Logic ---
def process_message_with_tools(messages: List[Dict]) -> str:
    """
    Processes a list of messages, makes a call to the Chat Completions API,
    and handles tool calls if the model requests them.

    Args:
        messages: The current conversation history.

    Returns:
        The final text response from the assistant.
    """
    print("Step 1: Calling the Chat Completions API...")
    response = client.chat.completions.create(
        model="gpt-4o-mini",  # A great, cost-effective model that supports tool calling
        messages=messages,
        tools=tools,
        tool_choice="auto",  # The model decides whether to use a tool or not.
    )
    response_message = response.choices[0].message

    # Step 2: Check if the model decided to use a tool.
    tool_calls = response_message.tool_calls
    if tool_calls:
        print("Step 2a: Model requested a tool call.")
        # The assistant's message with the tool call needs to be added to the history.
        st.session_state.messages.append(response_message)

        # Step 3: Execute the tool calls.
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            function_to_call = available_tools[function_name]
            function_args = json.loads(tool_call.function.arguments)

            # Call the actual Python function
            function_response = function_to_call(**function_args)

            # Step 4: Add the tool's output to the conversation history.
            st.session_state.messages.append(
                {
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_name,
                    "content": str(function_response),  # The output must be a string
                }
            )
            print(f"Step 4: Appended tool response to history: {function_response}")

        # Step 5: Make a second API call with the tool's response now in the history.
        # The model will use this new information to formulate its final, user-facing answer.
        print("Step 5: Making a second API call with tool output...")
        second_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=st.session_state.messages,
        )
        return second_response.choices[0].message.content

    # If no tool was called, just return the model's first response.
    else:
        print("Step 2b: No tool call requested. Returning direct response.")
        return response_message.content


# --- 6. Main Streamlit Application UI ---
def main():
    """
    The main function that defines the Streamlit user interface and application logic.
    """
    st.title("üìù Contract Agent (Chat Completions API)")
    st.write("Ask me everything about the contracts.")

    # Display the existing conversation messages. We skip the initial system message.
    for message in st.session_state.messages:
        if message["role"] != "system" and message["role"] != "tool":
            with st.chat_message(message["role"]):
                st.write(message["content"])

    # Create the chat input box at the bottom of the screen.
    if user_input := st.chat_input("Write here..."):
        # Add user's message to state and display it
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.write(user_input)

        # Get and display the assistant's response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = process_message_with_tools(st.session_state.messages)
            st.write(response)

        # Add the final assistant response to the message history for context in the next turn.
        st.session_state.messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    main()